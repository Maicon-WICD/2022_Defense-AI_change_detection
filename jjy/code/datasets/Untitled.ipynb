{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b909b74-f1dc-4639-993a-98a87e88b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff1741df-635c-4836-ad48-08a5ee91bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#util\n",
    "def is_image_file(filename):\n",
    "    IMG_EXTENSIONS = [\n",
    "        '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "        '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', 'tif', 'tiff'\n",
    "    ]\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def make_dataset(dir, max_dataset_size=float(\"inf\")):\n",
    "    images = []\n",
    "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "\n",
    "    for root, _, fnames in sorted(os.walk(dir)):\n",
    "        for fname in fnames:\n",
    "            if is_image_file(fname):\n",
    "                path = os.path.join(root, fname)\n",
    "                images.append(path)\n",
    "    return images[:min(max_dataset_size, len(images))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84b41e3e-98f2-41ce-8b0a-84a41aa94806",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/opt/ml/workspace/defense_dataset/train'\n",
    "A_paths = sorted(make_dataset(os.path.join(data_path, 'x')))\n",
    "L_paths = sorted(make_dataset(os.path.join(data_path, 'y')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14056193-e9de-4c2b-bf1c-0eac77bbae2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(754, 754, 3) (754, 754, 3)\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(A_paths[0])\n",
    "img = np.asarray(img)\n",
    "img_shape = img.shape\n",
    "img1 = Image.fromarray(img[:, :img_shape[1]//2, :])\n",
    "img2 = Image.fromarray(img[:, img_shape[1]//2:, :])\n",
    "img1 = np.asarray(img1)\n",
    "img2 = np.asarray(img2)\n",
    "img1 = cv2.resize(img1, dsize = (754, 754), interpolation=cv2.INTER_CUBIC)\n",
    "img2 = cv2.resize(img2, dsize = (754, 754), interpolation=cv2.INTER_CUBIC)\n",
    "print(img1.shape, img2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d579003f-5f7e-436d-a4f5-b946ceda5440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(754, 1508)\n",
      "(744, 744)\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(L_paths[0])\n",
    "img = np.asarray(img)\n",
    "img_shape = img.shape\n",
    "print(img_shape)\n",
    "label1 = Image.fromarray(img[:, :img_shape[1]//2])\n",
    "label2 = Image.fromarray(img[:, img_shape[1]//2:])\n",
    "label1 = label1.crop([5,5,(img_shape[1]//2)-5,img_shape[0]-5])\n",
    "label1 = np.asarray(label1)\n",
    "label2 = label2.crop([5,5,(img_shape[1]//2)-5,img_shape[0]-5])\n",
    "label2 = np.asarray(label2)\n",
    "# label = label1 + label2\n",
    "# print(label1.shape, label2.shape)\n",
    "# print(label.shape)\n",
    "# print(np.where(label == 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a108855f-42ef-42fd-9afd-b76d9d426d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[742 743 742 743 742 743 742 743 742 743 742 743 742 743 742 743 742 743\n",
      " 742 743 742 743 742 743 742 743 742 743 742 743 742 743 742 743 742 743\n",
      " 742 743 742 743 742 743 742 743 742 743 742 743 742 743 742 743 742 743\n",
      " 742 743 742 743 742 743 742 743 742 743 743 743 743 743 743 743 743 743\n",
      " 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743\n",
      " 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743\n",
      " 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743\n",
      " 743 743 743 743 743 743] [ 0  0  1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9 10 10 11 11\n",
      " 12 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21 22 22 23 23\n",
      " 24 24 25 25 26 26 27 27 28 28 29 29 30 30 31 31 32 33 34 35 36 37 38 39\n",
      " 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63\n",
      " 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87\n",
      " 88 89 90 91 92 93 94 95 96 97 98 99]\n",
      "3_error /opt/ml/workspace/defense_dataset/train/y/2016_KSG_JKG_000123.png\n",
      "[743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743] [357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374]\n",
      "3_error /opt/ml/workspace/defense_dataset/train/y/2016_WSN_2LB_000141.png\n",
      "[366 365 366 ... 371 372 371] [252 253 253 ... 474 474 475]\n",
      "1_error /opt/ml/workspace/defense_dataset/train/y/2017_KAG_2LB_000912.png\n",
      "[742 743 742 743 742 743 742 743 742 743 742 743 742 743 742 743 742 743\n",
      " 742 743 742 743 742 743 742 743 742 743 742 743 742 743 742 743 742 743\n",
      " 742 743 742 743 742 743 742 743 742 743 742 743 742 743 742 743 742 743\n",
      " 742 743 742 743 742 743 742 743 742 743 742 743 742 743 742 743 742 743\n",
      " 742 743 742 743 742 743 743 743 743 743 743 743 743 743 743 743 743 743\n",
      " 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743\n",
      " 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743\n",
      " 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743 743\n",
      " 743 743 743 743 743 743 743 743 743 743] [  0   0   1   1   2   2   3   3   4   4   5   5   6   6   7   7   8   8\n",
      "   9   9  10  10  11  11  12  12  13  13  14  14  15  15  16  16  17  17\n",
      "  18  18  19  19  20  20  21  21  22  22  23  23  24  24  25  25  26  26\n",
      "  27  27  28  28  29  29  30  30  31  31  32  32  33  33  34  34  35  35\n",
      "  36  36  37  37  38  38  39  40  41  42  43  44  45  46  47  48  49  50\n",
      "  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68\n",
      "  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86\n",
      "  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104\n",
      " 105 106 107 108 109 110 111 112 113 114]\n",
      "1_error /opt/ml/workspace/defense_dataset/train/y/2017_SPG_2LB_000441.png\n",
      "[743 743 743 743 743 743 743 743 743 743 743 742 743 742 743 742 743 742\n",
      " 743 742 743 742 743 742 743 742 743 742 743 742 743 742 743 741 742 743\n",
      " 741 742 743 741 742 743 741 742 743 741 742 743 741 742 743 741 742 743\n",
      " 741 742 743 741 742 743 741 742 743 740 741 742 743 740 741 742 743 740\n",
      " 741 742 743 740 741 742 743 740 741 742 743 740 741 742 743 740 741 742\n",
      " 743 740 741 742 743 740 741 742 743 740 741 742 743 740 741 742 743 739\n",
      " 740 741 742 743 739 740 741 742 743 739 740 741 742 743 739 740 741 742\n",
      " 743 739 740 741 742 743 739 740 741 742 743] [693 694 695 696 697 698 699 700 701 702 703 704 704 705 705 706 706 707\n",
      " 707 708 708 709 709 710 710 711 711 712 712 713 713 714 714 715 715 715\n",
      " 716 716 716 717 717 717 718 718 718 719 719 719 720 720 720 721 721 721\n",
      " 722 722 722 723 723 723 724 724 724 725 725 725 725 726 726 726 726 727\n",
      " 727 727 727 728 728 728 728 729 729 729 729 730 730 730 730 731 731 731\n",
      " 731 732 732 732 732 733 733 733 733 734 734 734 734 735 735 735 735 736\n",
      " 736 736 736 736 737 737 737 737 737 738 738 738 738 738 739 739 739 739\n",
      " 739 740 740 740 740 740 741 741 741 741 741]\n",
      "3_error /opt/ml/workspace/defense_dataset/train/y/2017_WSN_2LB_000118.png\n",
      "[743 743 743] [27 28 29]\n",
      "3_error /opt/ml/workspace/defense_dataset/train/y/2018_KSG_JJG_000069.png\n",
      "[371 372 371 ... 475 476 477] [701 701 702 ... 743 743 743]\n",
      "2_error /opt/ml/workspace/defense_dataset/train/y/2019_JNG_1LB_000005.png\n",
      "[562 563 564 ... 577 576 577] [454 454 454 ... 579 580 580]\n",
      "1_error /opt/ml/workspace/defense_dataset/train/y/2019_KBG_3LB_000073.png\n",
      "[743 743 743 ... 741 742 743] [608 609 610 ... 701 701 701]\n",
      "1_error /opt/ml/workspace/defense_dataset/train/y/2019_KDG_2LB_000871.png\n",
      "[319 320 317 ... 418 419 420] [434 434 435 ... 743 743 743]\n",
      "2_error /opt/ml/workspace/defense_dataset/train/y/2019_KSG_2LB_000374.png\n",
      "[743] [148]\n",
      "3_error /opt/ml/workspace/defense_dataset/train/y/2019_NWG_3LB_000281.png\n",
      "[276 277 278 ... 324 319 320] [ 0  0  0 ... 30 31 31]\n",
      "2_error /opt/ml/workspace/defense_dataset/train/y/2019_SMG_1LB_000027.png\n",
      "[0 1 0 ... 0 1 0] [246 246 247 ... 338 338 339]\n",
      "1_error /opt/ml/workspace/defense_dataset/train/y/2019_WSN_2LB_000047.png\n"
     ]
    }
   ],
   "source": [
    "for L_path in L_paths:\n",
    "    img = Image.open(L_path)\n",
    "    img = np.asarray(img)\n",
    "    label1 = Image.fromarray(img[:, :img_shape[1]//2])\n",
    "    label2 = Image.fromarray(img[:, img_shape[1]//2:])\n",
    "    label1 = label1.crop([5,5,(img_shape[1]//2)-5,img_shape[0]-5])\n",
    "    label1 = np.asarray(label1)\n",
    "    label2 = label2.crop([5,5,(img_shape[1]//2)-5,img_shape[0]-5])\n",
    "    label2 = np.asarray(label2)\n",
    "    err_1_height, err_1_width = np.where(label1 == 1)\n",
    "    err_2_height, err_2_width = np.where(label2 == 2)\n",
    "    err_3_height, err_3_width = np.where(label1 == 3)\n",
    "    \n",
    "#     label1 = cv2.resize(label1, dsize = (754, 754), interpolation=cv2.INTER_NEAREST)\n",
    "#     label2 = cv2.resize(label2, dsize = (754, 754), interpolation=cv2.INTER_NEAREST)\n",
    "    label = label1 + label2\n",
    "#     err_1_height, err_1_width = np.where(label1 == 1)\n",
    "#     err_2_height, err_2_width = np.where(label2 == 2)\n",
    "#     err_3_height, err_3_width = np.where(label1 == 3)\n",
    "    if err_1_height.size != 0 or err_1_width.size != 0:\n",
    "        print(err_1_width, err_1_height)\n",
    "        print(f\"1_error {L_path}\")\n",
    "    if err_2_height.size != 0 or err_2_width.size != 0:\n",
    "        print(err_2_width, err_2_height)\n",
    "        print(f\"2_error {L_path}\")\n",
    "    if err_3_height.size != 0 or err_3_width.size != 0:\n",
    "        print(err_3_width, err_3_height)\n",
    "        print(f\"3_error {L_path}\")\n",
    "# #     height, width = np.where(label >= 4)\n",
    "#     if height.size != 0 or width.size != 0:\n",
    "#         print(L_path)\n",
    "# #         print(height, width)\n",
    "#         print(label1[height[0], width[0]])\n",
    "#         print(label2[height[0], width[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "70168748-83ab-41e9-84dd-394d329f800d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/workspace/defense_dataset/train/y/2016_YDP_JJG_000075.png\n",
      "2\n",
      "1\n",
      "/opt/ml/workspace/defense_dataset/train/y/2018_SCG_2LB_000288.png\n",
      "2\n",
      "1\n",
      "/opt/ml/workspace/defense_dataset/train/y/2018_SCG_2LB_000292.png\n",
      "2\n",
      "1\n",
      "/opt/ml/workspace/defense_dataset/train/y/2019_JNG_1LB_000129.png\n",
      "2\n",
      "1\n",
      "/opt/ml/workspace/defense_dataset/train/y/2019_KSG_2LB_000589.png\n",
      "2\n",
      "1\n",
      "/opt/ml/workspace/defense_dataset/train/y/2019_KSG_KNI_000461.png\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for L_path in L_paths:\n",
    "    img = Image.open(L_path)\n",
    "    img = np.asarray(img)\n",
    "    label1 = Image.fromarray(img[:, :img_shape[1]//2])\n",
    "    label2 = Image.fromarray(img[:, img_shape[1]//2:])\n",
    "    label1 = label1.crop([5,5,(img_shape[1]//2)-5,img_shape[0]-5])\n",
    "    label1 = np.asarray(label1)\n",
    "    label2 = label2.crop([5,5,(img_shape[1]//2)-5,img_shape[0]-5])\n",
    "    label2 = np.asarray(label2)\n",
    "#     err_1_height, err_1_width = np.where(label1 == 1)\n",
    "#     err_2_height, err_2_width = np.where(label2 == 2)\n",
    "#     err_3_height, err_3_width = np.where(label1 == 3)\n",
    "    label1 = cv2.resize(label1, dsize = (754, 754), interpolation=cv2.INTER_NEAREST)\n",
    "    label2 = cv2.resize(label2, dsize = (754, 754), interpolation=cv2.INTER_NEAREST)\n",
    "    label1 = Image.fromarray(label1)\n",
    "    label2 = Image.fromarray(label2)\n",
    "    label1 = label1.crop([5,5,749,749])\n",
    "    label1 = np.asarray(label1)\n",
    "    label2 = label2.crop([5,5,749,749])\n",
    "    label2 = np.asarray(label2)\n",
    "    h, w = np.where(label2 == 3)\n",
    "    label2 = label2.copy()\n",
    "    label2[h, w] = 0\n",
    "    label = label1 + label2\n",
    "#     err_1_height, err_1_width = np.where(label1 == 1)\n",
    "#     err_2_height, err_2_width = np.where(label2 == 2)\n",
    "#     err_3_height, err_3_width = np.where(label1 == 3)\n",
    "#     if err_1_height.size != 0 or err_1_width.size != 0:\n",
    "#         print(err_1_width, err_1_height)\n",
    "#         print(f\"1_error {L_path}\")\n",
    "#     if err_2_height.size != 0 or err_2_width.size != 0:\n",
    "#         print(err_2_width, err_2_height)\n",
    "#         print(f\"2_error {L_path}\")\n",
    "#     if err_3_height.size != 0 or err_3_width.size != 0:\n",
    "#         print(err_3_width, err_3_height)\n",
    "#         print(f\"3_error {L_path}\")\n",
    "    height, width = np.where(label >= 3)\n",
    "    if height.size != 0 or width.size != 0:\n",
    "        print(L_path)\n",
    "#         print(height, width)\n",
    "        print(label1[height[0], width[0]])\n",
    "        print(label2[height[0], width[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3993ed8-5577-4807-adfb-114aec449311",
   "metadata": {},
   "source": [
    "- 1 error\n",
    "  - 2017_KAG_2LB_000912.png\n",
    "  - 2019_KBG_3LB_000073.png\n",
    "  - 2019_KDG_2LB_000871.png\n",
    "  - 2019_WSN_2LB_000047.png\n",
    "\n",
    "- 2 error\n",
    "  - 2019_JNG_1LB_000005.png\n",
    "  - 2019_KSG_2LB_000374.png\n",
    "  - 2019_SMG_1LB_000027.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "565f4f24-8d6d-4380-a991-2ac6d734c9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(754, 1508)\n",
      "(array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64))\n",
      "(array([455, 456, 456, ..., 647, 647, 647]), array([1201, 1200, 1201, ...,  857,  858,  859]))\n",
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "img = cv2.imread(L_paths[300], cv2.IMREAD_GRAYSCALE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "457a357f-57c9-4afc-8b3b-6465c021c686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3, 1, 3,  ..., 3, 3, 3],\n",
      "         [1, 0, 3,  ..., 3, 3, 3],\n",
      "         [1, 0, 3,  ..., 3, 3, 1],\n",
      "         ...,\n",
      "         [3, 1, 3,  ..., 0, 3, 1],\n",
      "         [3, 3, 3,  ..., 0, 0, 1],\n",
      "         [3, 3, 0,  ..., 3, 3, 3]]])\n",
      "torch.Size([1, 64, 64])\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=4,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "mask = model(torch.ones([1, 3, 64, 64]))\n",
    "a = mask.argmax(1)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eadf0e-72d8-4436-bb9e-83b19daed115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
